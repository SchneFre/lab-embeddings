{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings\n",
    "\n",
    "This lab is designed to help you solidify your understanding of embeddings by applying them to tasks like semantic similarity, clustering, and building a semantic search system.\n",
    "\n",
    "### Tasks:\n",
    "- Task 1: Semantic Similarity Comparison\n",
    "- Task 2: Document Clustering\n",
    "- Task 3: Enhance the Semantic Search System\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Semantic Similarity Comparison\n",
    "### Objective:\n",
    "Compare semantic similarity between pairs of sentences using cosine similarity and embeddings.\n",
    "\n",
    "### Steps:\n",
    "1. Load a pre-trained Sentence Transformer model.\n",
    "2. Encode the sentence pairs.\n",
    "3. Compute cosine similarity for each pair.\n",
    "\n",
    "### Dataset:\n",
    "- \"A dog is playing in the park.\" vs. \"A dog is running in a field.\"\n",
    "- \"I love pizza.\" vs. \"I enjoy ice cream.\"\n",
    "- \"What is AI?\" vs. \"How does a computer learn?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba82e852d6f94e00912ca1c09bc638b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c0d8336c17a43c1a6cf39822afff336",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffbac0c76ea24706a81236f068b56da3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d29c465831b8434c9bdc9df887bfdf95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b86637dab934615bdcb3946c09988f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "981f454119334f89b8bed8566a35d559",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e70db3cf81eb4f66a78f8f896af4ae22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9bea36c365e457e9b25dc89dc1cbbeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fd7328c873c43d387283e77689eb035",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6e548e1e345458f8a6a173f36f48bc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ea863f04c134bb2947e072fbad9086c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: A dog is playing in the park.\n",
      "Sentence 2: A dog is running in a field.\n",
      "Cosine Similarity: 0.5220\n",
      "--------------------------------------------------\n",
      "Sentence 1: I love pizza.\n",
      "Sentence 2: I enjoy ice cream.\n",
      "Cosine Similarity: 0.5281\n",
      "--------------------------------------------------\n",
      "Sentence 1: What is AI?\n",
      "Sentence 2: How does a computer learn?\n",
      "Cosine Similarity: 0.3194\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load pre-trained model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Sentence pairs\n",
    "sentence_pairs = [\n",
    "    (\"A dog is playing in the park.\", \"A dog is running in a field.\"),\n",
    "    (\"I love pizza.\", \"I enjoy ice cream.\"),\n",
    "    (\"What is AI?\", \"How does a computer learn?\")\n",
    "]\n",
    "\n",
    "# Compute similarities\n",
    "for sent1, sent2 in sentence_pairs:\n",
    "    # Encode sentences\n",
    "    embedding1 = model.encode(sent1)\n",
    "    embedding2 = model.encode(sent2)\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    similarity = cosine_similarity(\n",
    "        [embedding1],\n",
    "        [embedding2]\n",
    "    )[0][0]\n",
    "\n",
    "    print(f\"Sentence 1: {sent1}\")\n",
    "    print(f\"Sentence 2: {sent2}\")\n",
    "    print(f\"Cosine Similarity: {similarity:.4f}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence_transformers\n",
      "  Downloading sentence_transformers-5.1.2-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence_transformers)\n",
      "  Downloading transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\friedrich\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sentence_transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\friedrich\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sentence_transformers) (2.8.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\friedrich\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sentence_transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\friedrich\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sentence_transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\friedrich\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sentence_transformers) (0.33.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\friedrich\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sentence_transformers) (10.4.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\friedrich\\appdata\\roaming\\python\\python39\\site-packages (from sentence_transformers) (4.14.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\friedrich\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\friedrich\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2025.5.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\friedrich\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\friedrich\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\friedrich\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\friedrich\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\friedrich\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\friedrich\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (3.1.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\friedrich\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tqdm->sentence_transformers) (0.4.6)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence_transformers)\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\friedrich\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\friedrich\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2025.11.3)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers<5.0.0,>=4.41.0->sentence_transformers)\n",
      "  Downloading tokenizers-0.22.2-cp39-abi3-win_amd64.whl.metadata (7.4 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers<5.0.0,>=4.41.0->sentence_transformers)\n",
      "  Downloading safetensors-0.7.0-cp38-abi3-win_amd64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\friedrich\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn->sentence_transformers) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\friedrich\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn->sentence_transformers) (3.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\friedrich\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\friedrich\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\friedrich\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\friedrich\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\friedrich\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\friedrich\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2025.8.3)\n",
      "Downloading sentence_transformers-5.1.2-py3-none-any.whl (488 kB)\n",
      "Downloading transformers-4.57.3-py3-none-any.whl (12.0 MB)\n",
      "   ---------------------------------------- 0.0/12.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/12.0 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/12.0 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/12.0 MB 1.1 MB/s eta 0:00:11\n",
      "   -- ------------------------------------- 0.8/12.0 MB 1.2 MB/s eta 0:00:10\n",
      "   --- ------------------------------------ 1.0/12.0 MB 1.2 MB/s eta 0:00:10\n",
      "   ---- ----------------------------------- 1.3/12.0 MB 1.2 MB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 1.6/12.0 MB 1.2 MB/s eta 0:00:09\n",
      "   ------ --------------------------------- 1.8/12.0 MB 1.2 MB/s eta 0:00:09\n",
      "   ------ --------------------------------- 2.1/12.0 MB 1.2 MB/s eta 0:00:09\n",
      "   ------- -------------------------------- 2.4/12.0 MB 1.2 MB/s eta 0:00:09\n",
      "   -------- ------------------------------- 2.6/12.0 MB 1.2 MB/s eta 0:00:09\n",
      "   -------- ------------------------------- 2.6/12.0 MB 1.2 MB/s eta 0:00:09\n",
      "   --------- ------------------------------ 2.9/12.0 MB 1.2 MB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 3.1/12.0 MB 1.2 MB/s eta 0:00:08\n",
      "   ----------- ---------------------------- 3.4/12.0 MB 1.2 MB/s eta 0:00:08\n",
      "   ------------ --------------------------- 3.7/12.0 MB 1.2 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 3.9/12.0 MB 1.2 MB/s eta 0:00:07\n",
      "   ------------- -------------------------- 4.2/12.0 MB 1.2 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 4.5/12.0 MB 1.2 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 4.7/12.0 MB 1.2 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 5.0/12.0 MB 1.2 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 5.2/12.0 MB 1.2 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 5.5/12.0 MB 1.2 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 5.5/12.0 MB 1.2 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 5.8/12.0 MB 1.2 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 6.0/12.0 MB 1.2 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 6.3/12.0 MB 1.2 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 6.6/12.0 MB 1.2 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 6.8/12.0 MB 1.2 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 7.1/12.0 MB 1.2 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 7.3/12.0 MB 1.2 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 7.6/12.0 MB 1.2 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 7.9/12.0 MB 1.2 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 8.1/12.0 MB 1.2 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 8.1/12.0 MB 1.2 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 8.4/12.0 MB 1.2 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 8.7/12.0 MB 1.2 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 8.9/12.0 MB 1.2 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 9.2/12.0 MB 1.2 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 9.4/12.0 MB 1.2 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 9.7/12.0 MB 1.2 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 10.0/12.0 MB 1.2 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 10.2/12.0 MB 1.2 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 10.5/12.0 MB 1.2 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 10.7/12.0 MB 1.2 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 11.0/12.0 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.0/12.0 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.3/12.0 MB 1.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.5/12.0 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.8/12.0 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.0/12.0 MB 1.2 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "   ---------------------------------------- 0.0/566.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/566.1 kB ? eta -:--:--\n",
      "   ------------------ --------------------- 262.1/566.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 566.1/566.1 kB 1.2 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.7.0-cp38-abi3-win_amd64.whl (341 kB)\n",
      "Downloading tokenizers-0.22.2-cp39-abi3-win_amd64.whl (2.7 MB)\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.3/2.7 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.5/2.7 MB 1.1 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 0.8/2.7 MB 1.1 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 1.0/2.7 MB 1.1 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 1.3/2.7 MB 1.2 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 1.6/2.7 MB 1.1 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 1.8/2.7 MB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 2.1/2.7 MB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 2.1/2.7 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 2.4/2.7 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.7/2.7 MB 1.1 MB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers, sentence_transformers\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.33.0\n",
      "    Uninstalling huggingface-hub-0.33.0:\n",
      "      Successfully uninstalled huggingface-hub-0.33.0\n",
      "Successfully installed huggingface-hub-0.36.0 safetensors-0.7.0 sentence_transformers-5.1.2 tokenizers-0.22.2 transformers-4.57.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gradio-client 1.3.0 requires websockets<13.0,>=10.0, but you have websockets 15.0.1 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "- Which sentence pairs are the most semantically similar? Why?\n",
    "- Can you think of cases where cosine similarity might fail to capture true semantic meaning?\n",
    "\n",
    "\n",
    "Sentence 1: I love pizza.\n",
    "Sentence 2: I enjoy ice cream.\n",
    "Cosine Similarity: 0.5281"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Document Clustering\n",
    "### Objective:\n",
    "Cluster a set of text documents into similar groups based on their embeddings.\n",
    "\n",
    "### Steps:\n",
    "1. Encode the documents using Sentence Transformers.\n",
    "2. Use KMeans clustering to group the documents.\n",
    "3. Analyze the clusters for semantic meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Documents to cluster\n",
    "documents = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"How do I bake a chocolate cake?\",\n",
    "    \"What is the distance between Earth and Mars?\",\n",
    "    \"How do I change a flat tire on a car?\",\n",
    "    \"What is the best way to learn Python?\",\n",
    "    \"How do I fix a leaky faucet?\"\n",
    "]\n",
    "\n",
    "# Encode documents\n",
    "embeddings = model.encode(documents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform KMeans clustering\n",
    "\n",
    "#YOUR CODE HERE\n",
    "\n",
    "# Apply KMeans clustering\n",
    "k = 3  # Number of clusters\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "labels = kmeans.fit_predict(embeddings)\n",
    "\n",
    "# Analyze clusters\n",
    "clusters = {}\n",
    "for doc, label in zip(documents, labels):\n",
    "    clusters.setdefault(label, []).append(doc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster 2:\n",
      " - What is the capital of France?\n",
      " - What is the best way to learn Python?\n",
      "\n",
      "Cluster 0:\n",
      " - How do I bake a chocolate cake?\n",
      " - What is the distance between Earth and Mars?\n",
      "\n",
      "Cluster 1:\n",
      " - How do I change a flat tire on a car?\n",
      " - How do I fix a leaky faucet?\n"
     ]
    }
   ],
   "source": [
    "# Print cluster assignments\n",
    "\n",
    "#YOUR CODE HERE\n",
    "\n",
    "for cluster_id, docs in clusters.items():\n",
    "    print(f\"\\nCluster {cluster_id}:\")\n",
    "    for doc in docs:\n",
    "        print(f\" - {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "- How many clusters make the most sense? Why?\n",
    "- Examine the documents in each cluster. Are they semantically meaningful? Can you assign a semantic \"theme\" to each cluster?\n",
    "- Try this exercise with a larger dataset of your choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Semantic Search System\n",
    "### Objective:\n",
    "Create a semantic search engine:\n",
    "A user provides a query and you search the dataset for semantically relevant documents to return. Return the top 5 results.\n",
    "\n",
    "### Dataset:\n",
    "- Use the following set of documents:\n",
    "    - \"What is the capital of France?\"\n",
    "    - \"How do I bake a chocolate cake?\"\n",
    "    - \"What is the distance between Earth and Mars?\"\n",
    "    - \"How do I change a flat tire on a car?\"\n",
    "    - \"What is the best way to learn Python?\"\n",
    "    - \"How do I fix a leaky faucet?\"\n",
    "    - \"What are the best travel destinations in Europe?\"\n",
    "    - \"How do I set up a local server?\"\n",
    "    - \"What is quantum computing?\"\n",
    "    - \"How do I build a mobile app?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Documents dataset\n",
    "documents = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"How do I bake a chocolate cake?\",\n",
    "    \"What is the distance between Earth and Mars?\",\n",
    "    \"How do I change a flat tire on a car?\",\n",
    "    \"What is the best way to learn Python?\",\n",
    "    \"How do I fix a leaky faucet?\",\n",
    "    \"What are the best travel destinations in Europe?\",\n",
    "    \"How do I set up a local server?\",\n",
    "    \"What is quantum computing?\",\n",
    "    \"How do I build a mobile app?\"\n",
    "]\n",
    "\n",
    "# Compute document embeddings\n",
    "doc_embeddings = model.encode(documents)\n",
    "#YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the search function\n",
    "#This function should encode the user query and return the top N documents that most resemble it\n",
    "def semantic_search(query, documents, doc_embeddings, top_k=5):\n",
    "    # Encode query\n",
    "    query_embedding = model.encode(query)\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    similarities = cosine_similarity(\n",
    "        [query_embedding],\n",
    "        doc_embeddings\n",
    "    )[0]\n",
    "\n",
    "    # Get top-k results\n",
    "    top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "\n",
    "    # Return results\n",
    "    results = [\n",
    "        (documents[i], similarities[i])\n",
    "        for i in top_indices\n",
    "    ]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('What is quantum computing?', np.float32(0.43524778)),\n",
       " ('What is the best way to learn Python?', np.float32(0.31878257)),\n",
       " ('How do I build a mobile app?', np.float32(0.110440716)),\n",
       " ('How do I set up a local server?', np.float32(0.091126524)),\n",
       " ('What are the best travel destinations in Europe?', np.float32(0.0906478))]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the search function\n",
    "query = \"Explain programming languages.\"\n",
    "semantic_search(query, documents, doc_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "- What are the top-ranked results for the given queries?\n",
    "- How can you improve the ranking explanation for users?\n",
    "- Try this approach with a larger dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
